{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46847e74-1cc5-407e-9fbc-e8270abc9295",
   "metadata": {},
   "source": [
    "# JAX Intro\n",
    "This notebook is a brief introduction to JAX. After working through this notebook, you will have a basic idea of \n",
    "1. What JAX is\n",
    "2. Why it's useful\n",
    "3. How to use it at a basic level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c52055-fb8d-41f8-8552-cd26228dbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # The numpy api implemented in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935ea81-195f-4ccb-9ea7-9d97de9ed8f4",
   "metadata": {},
   "source": [
    "## Why use JAX?\n",
    "![](https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png)\n",
    "\n",
    "In Google's words:\n",
    ">JAX is Autograd and XLA, brought together for high-performance numerical computing, including large-scale machine learning research.\n",
    "\n",
    "That's great if you know what Autograd and XLA are. Let's assume we don't.\n",
    "\n",
    "### Autograd\n",
    "[Autograd](https://github.com/hips/autograd) is an automatic differentiation library for pure Python and NumPy.\n",
    "At a basic level, this means we can efficiently compute gradients in our code\n",
    "\n",
    "### XLA\n",
    "[XLA](https://github.com/openxla/xla) stands for Accelerated Linear Algebra. This is a compiler for common machine learning libraries in Python that allows us to compile our code for GPUs, CPUs, NPUs, TPUs, and whatever acronyms we come up with next for new accelerators.\n",
    "\n",
    "### Takeaway\n",
    "JAX gives us fast, compiled code that can run on accelerators and efficiently compute gradients. This is _extremely_ useful not only for machine learning and AI, but also for inference, simulation, etc. in the sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5285d-7bd3-4b1f-9863-e565468b4b7e",
   "metadata": {},
   "source": [
    "## Some JAX Basics\n",
    "One of the nicest things about JAX is that it implements the [NumPy API](https://jax.readthedocs.io/en/latest/jax.numpy.html).\n",
    "We imported this earlier as  ```jnp```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfeb3cf-8132-4cdd-a87f-59f7ce06469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = jnp.ones((3, 3))\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2b3db-90a6-44c6-92e1-340d1c7e6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "twos = jnp.ones((3, 3)) * 2\n",
    "twos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0714b-e2f6-4909-b404-e7ba78f5f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = ones + twos\n",
    "threes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996f399-564d-4ef4-a477-dba3b9d92ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes.devices()  # CUDA, CPU, TPU, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2df59-ebd0-4261-88f2-7ad5236eb4a4",
   "metadata": {},
   "source": [
    "As you can see, we can manipulate arrays with JAX (mostly) as we expect.\n",
    "I will show an example of how things in JAX are a bit different.\n",
    "Let's say we want to change one of the array elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791ea69-dd3b-4e14-94cd-f263d75d89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes[0, 1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9aff2-fe87-4a8e-823c-06f7f96db98a",
   "metadata": {},
   "source": [
    "**We get an error!**\n",
    "\n",
    "Why? It's because JAX requires us to write *functional* code. \n",
    "When I say functional, I mean in the sense of functional programming.\n",
    "The above operation is an example of a *side effect*, something not allowed in functional programming.\n",
    "We must instead have a function that modifies the existing array and returns a new one.\n",
    "Luckily, JAX provides these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ef890-f4b4-4fc7-9dda-7f659d441dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes.at[0, 1].add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0f784-64b1-430f-b4fb-f3dda2dbfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But this is still the same\n",
    "threes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e23ab-a747-456e-af82-3413a238dcf1",
   "metadata": {},
   "source": [
    "To be **functional**, we must return the value to some output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60becc-e8a5-4fa8-a6d0-697d9b1cca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = threes.at[0, 1].add(1)\n",
    "threes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9efbf5-0eb3-4220-8750-f012ffb64495",
   "metadata": {},
   "source": [
    "This pattern will repeat throughout JAX codebases.\n",
    "Most of JAX's objects (like these arrays) are what we would call **immutable**.\n",
    "Think of tuples in pure Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbcf68a-e7d5-4a60-984f-7678989b100a",
   "metadata": {},
   "source": [
    "## Taking Gradients in JAX\n",
    "We will use `jax.grad` to compute a gradient of a function written with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3b6c7-a22a-4ea0-ba30-83aef9062262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(x):\n",
    "    return jnp.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868c565-5f89-4abe-89d9-168984cb08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_squares_dx = jax.grad(sum_of_squares)\n",
    "\n",
    "x = jnp.asarray([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "print(sum_of_squares(x))\n",
    "\n",
    "print(sum_of_squares_dx(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc271fd6-13f0-4bb0-b2ba-2fcf65ae9c62",
   "metadata": {},
   "source": [
    "We can also get back the value and gradient at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7572cd-f2a2-4762-b9ac-77bb935013d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_squares_val_dx = jax.value_and_grad(sum_of_squares)\n",
    "\n",
    "x = jnp.asarray([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "print(sum_of_squares(x))\n",
    "\n",
    "print(sum_of_squares_val_dx(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7880c1-5777-4d41-9cb6-59bce1d2dbb4",
   "metadata": {},
   "source": [
    "## JIT (Just in Time) Compilation\n",
    "We can also compile JAX functions using XLA.\n",
    "To do this, JAX builds an intermediate representation of the code called a *jaxpr* (JAX exPRession).\n",
    "The XLA compiler then reads this *jaxpr*.\n",
    "\n",
    "Let's see an example of a jaxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0adad3-2b9f-40d8-9b6f-3c2eaaef9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(sum_of_squares)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e1417-e488-4db7-900c-332447fa06fb",
   "metadata": {},
   "source": [
    "Notice how the above is broken down into a series of primitive operations.\n",
    "\n",
    "Now, let's look at compiling the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362840d4-cebe-4e4b-aea5-c54a2d7f153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_squares_jit = jax.jit(sum_of_squares)\n",
    "sum_of_squares_jit(x).block_until_ready()\n",
    "z = x.repeat(1_000_000, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47623d10-218b-445c-86a1-cba0d1102f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sum_of_squares_jit(z).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2071dd-0d35-4625-ab87-5c9b9a17074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sum_of_squares(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965348a1-d2ca-44c9-8ddd-9b7cd7e5db39",
   "metadata": {},
   "source": [
    "A very silly example, but JAX does give us a speedup. This would be more evident if executed on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f74b2e-a3d1-4390-a532-af782e4e8dfa",
   "metadata": {},
   "source": [
    "## Automatic Vectorization\n",
    "You may be familiar with the idea of *vectorized computations*, a sub-class of *single instruction, multiple data* parallel processing.\n",
    "See the following example of array addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476486ec-f643-4b0b-a676-3dcdfb98ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = jnp.arange(5), jnp.arange(5)\n",
    "jax.make_jaxpr(lambda a, b: a * b)(\n",
    "    a, b\n",
    ")  # This is a single primitive! It is done in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924adef-6d41-40a1-8f91-377a87e78bea",
   "metadata": {},
   "source": [
    "### What's going on here?\n",
    "Our CPU (or GPU, TPU) have vector instruction sets. \n",
    "They know how to execute a single instruction (multiplication) on multiple data (the array elements) in parallel.\n",
    "The specifics of this are processor-specific and get complicated very quickly.\n",
    "\n",
    "![](./imgs/vectorization.png)\n",
    "\n",
    "(Image credit https://datascience.blog.wzb.eu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a5f97-9382-4dc1-907b-8512de41ac33",
   "metadata": {},
   "source": [
    "### What if we don't use a built-in vectorized function?\n",
    "Most NumPy array math and linear algebra routines are vectorized (as are the JAX counterparts), but what happens if you want to vectorize your own custom code?\n",
    "JAX has a utility to automatically do this: `jax.vmap`\n",
    "\n",
    "Below we have a function that takes a windowed, weighted average of an input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a207f-ea78-4129-8054-f49494610925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(x, w):\n",
    "    output = []\n",
    "    for i in range(1, len(x) - 1):\n",
    "        output.append(jnp.dot(x[i - 1 : i + 2], w))\n",
    "    return jnp.array(output)\n",
    "\n",
    "\n",
    "x = jnp.arange(5)\n",
    "w = jnp.array([2.0, 3.0, 4.0])\n",
    "\n",
    "convolve(x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae686c4a-cab6-42b6-9bb9-578e80d57622",
   "metadata": {},
   "source": [
    "Let's assume we actually have a batch of arrays and weights. We want to vectorize our function across this batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110c97a-76a3-4e0b-948c-65085fb0a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.stack([x, 2 * x])\n",
    "ws = jnp.stack([w, w])\n",
    "xs, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa22ebb-c804-424c-923f-5b86533fa274",
   "metadata": {},
   "source": [
    "It's not too difficult to manually update the code to be vectorized across the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56f1c4-e2cf-4324-800e-45ebe40b427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manually_vectorized_convolve(xs, ws):\n",
    "    output = []\n",
    "    for i in range(1, xs.shape[-1] - 1):\n",
    "        output.append(jnp.sum(xs[:, i - 1 : i + 2] * ws, axis=1))\n",
    "    return jnp.stack(output, axis=1)\n",
    "\n",
    "\n",
    "manually_vectorized_convolve(xs, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5b0c6-0dac-4381-a6fe-f133250c1ed8",
   "metadata": {},
   "source": [
    "These array gymnastics get tiring, and it's not always this easy. We can use JAX to make this trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2df85-86d8-40a1-a162-585ad9f7a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap_convolve = jax.vmap(convolve)\n",
    "\n",
    "vmap_convolve(xs, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e357e2-860c-47f6-8506-9c0073814957",
   "metadata": {},
   "source": [
    "We can also combine JIT with vmap to compile our vectorized function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38a5bb-c87d-49b8-83b1-22fe199fc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_vmap_convolve = jax.jit(vmap_convolve)\n",
    "\n",
    "jitted_vmap_convolve(xs, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c93fb-6001-443f-85f0-bed0c9acad69",
   "metadata": {},
   "source": [
    "## The Broader JAX Ecosystem\n",
    "- [Flax](https://flax.readthedocs.io/en/latest/): Neural networks with JAX\n",
    "- [Optax](https://optax.readthedocs.io/en/latest/): gradient processing and optimization library\n",
    "- [Equinox](https://docs.kidger.site/equinox/): Neural nets, JIT utilities, and more\n",
    "- [NumPyro](https://github.com/pyro-ppl/numpyro): probabilistic programming built on JAX\n",
    "- [BlackJAX](https://blackjax-devs.github.io/blackjax/): collection of samplers for Bayesian inference\n",
    "- [jax.scipy](https://jax.readthedocs.io/en/latest/jax.scipy.html): implementation of popular SciPy routines\n",
    "\n",
    "- [AwesomeJAX](https://github.com/n2cholas/awesome-jax): curated list of popular JAX packages"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,qmd"
  },
  "kernelspec": {
   "display_name": "Python [conda env:jax-tutorial]",
   "language": "python",
   "name": "conda-env-jax-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
